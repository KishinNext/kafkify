{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Kafkify","text":"<p>Kafkify is a lightweight, opinionated wrapper around <code>aiokafka</code> designed to simplify the integration of Kafka consumers and producers into Python applications, particularly those built with FastAPI.</p> <p>It provides a structured way to handle Kafka messages, manage configuration, and route messages to specific handlers based on topics and codes.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Easy Configuration: Uses Pydantic models for type-safe and clear configuration.</li> <li>Routing: Decorator-based routing similar to FastAPI for handling specific topics and message codes.</li> <li>Resilience: Built-in handling for rebalancing and offset management.</li> <li>FastAPI Integration: Designed to work seamlessly with FastAPI's lifespan events.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out the User Guide to understand how to use the Consumer and Producer components, or jump straight into the Examples to see them in action.</p>"},{"location":"consumer/","title":"Consumer","text":"<p>The Consumer component in Kafkify allows you to subscribe to Kafka topics and process incoming messages. It supports routing messages to specific handler functions based on the topic and an optional \"code\" within the message payload.</p>"},{"location":"consumer/#workflow","title":"Workflow","text":"<p>The following sequence diagram illustrates the lifecycle of the Consumer and how messages are processed.</p> <pre><code>sequenceDiagram\n    participant App as FastAPI App\n    participant Adapter as Kafka Consumer Adapter\n    participant Kafka as Kafka Broker\n    participant Router as Consumer Router\n    participant Handler as Message Handler\n\n    Note over App, Adapter: Startup Phase\n    App-&gt;&gt;Adapter: Init Config\n    App-&gt;&gt;Router: Register Handlers\n    App-&gt;&gt;Adapter: Start()\n    Adapter-&gt;&gt;Kafka: Connect &amp; Subscribe\n\n    Note over Adapter, Kafka: Listening Phase\n    loop Event Loop\n        Adapter-&gt;&gt;Kafka: Poll Messages\n        Kafka--&gt;&gt;Adapter: Return Batch\n\n        par Process Messages\n            Adapter-&gt;&gt;Router: Dispatch Message (Topic + Code)\n            Router-&gt;&gt;Handler: Execute Logic\n            Handler--&gt;&gt;Router: Result\n            Router--&gt;&gt;Adapter: Success\n            Adapter-&gt;&gt;Kafka: Commit Offset\n        end\n    end\n\n    Note over App, Adapter: Shutdown Phase\n    App-&gt;&gt;Adapter: Stop()\n    Adapter-&gt;&gt;Kafka: Close Connection</code></pre>"},{"location":"consumer/#usage","title":"Usage","text":"<p>To use the consumer:</p> <ol> <li>Configure: Create a configuration dictionary with attributes supported by aiokafka.</li> <li>Instantiate: Create an instance of <code>KafkaBaseConsumerAdapter</code> with the config.</li> <li>Register Handlers: Use the <code>@consumer.get</code> decorator or a <code>ConsumerRouter</code> to register functions to handle messages.</li> <li>Start: Call <code>await consumer.start()</code> to connect to Kafka.</li> <li>Listen: Call <code>await consumer.listen()</code> to start the consumption loop (usually in a background task).</li> <li>Stop: Call <code>await consumer.stop()</code> to gracefully shutdown.</li> </ol> <p>See the Consumer Example for a complete walkthrough.</p>"},{"location":"consumer/#message-structure-for-routing","title":"Message Structure for Routing","text":"<p>To use the <code>codes</code> filtering feature in your handlers, your Kafka messages must be deserialized into a dictionary and contain a <code>code</code> field.</p> <p>The Consumer Adapter inspects the message value:</p> <ol> <li>Checks if it is a dictionary.</li> <li>Extracts the value of the <code>code</code> key.</li> <li>Matches it against registered handlers.</li> </ol>"},{"location":"consumer/#example-payload","title":"Example Payload","text":"<p>For a handler registered with <code>@router.get({\"topic\": \"my-topic\", \"codes\": [\"USER_CREATED\"]})</code>, the message payload should look like this:</p> <pre><code>{\n  \"code\": \"USER_CREATED\",\n  \"data\": {\n    \"user_id\": 123,\n    \"email\": \"user@example.com\"\n  },\n  \"timestamp\": \"2023-10-27T10:00:00Z\"\n}\n</code></pre> <p>If the <code>code</code> field is missing or the value is not a dictionary, the message will be routed to the default handler for that topic (if one exists).</p> <p>Be careful</p> <p>If no handler is found, the event will be skipped and committed automatically.</p>"},{"location":"consumer/#components","title":"Components","text":""},{"location":"consumer/#configuration","title":"Configuration","text":"<p>The configuration is passed as a dictionary. It supports all arguments accepted by <code>aiokafka.AIOKafkaConsumer</code>.</p> <p>Common options include: - <code>bootstrap_servers</code>: list of 'host:port' strings or a single comma-separated string. - <code>group_id</code>: name of the consumer group. - <code>auto_offset_reset</code>: policy for initial offset ('earliest', 'latest'). - <code>enable_auto_commit</code>: whether to auto-commit offsets.</p> <p>Refer to the aiokafka documentation for a full list of parameters.</p>"},{"location":"consumer/#base-consumer","title":"Base Consumer","text":"<p>The <code>BaseConsumer</code> defines the interface for all consumer implementations.</p> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for a message bus that can consume from multiple topics and register handlers for message processing.</p> Source code in <code>src/consumers/domain/ports/base_consumer.py</code> <pre><code>class BaseConsumer(ABC):\n    \"\"\"\n    Abstract base class for a message bus that can consume from multiple topics\n    and register handlers for message processing.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Dict[str, Any],\n        key_deserializer: Optional[Callable] = None,\n        value_deserializer: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Initializes the BaseConsumer with configuration and deserializers.\n\n        Args:\n            config: The Kafka consumer configuration.\n            key_deserializer: Optional function to deserialize message keys.\n            value_deserializer: Optional function to deserialize message values.\n        \"\"\"\n        self.config = config\n        self._key_deserializer = key_deserializer\n        self._value_deserializer = value_deserializer\n\n    @abstractmethod\n    def get(\n        self,\n        config: Dict[str, Any],\n    ) -&gt; Callable[[Callable[[ConsumerRecord], Coroutine]], None]:\n        \"\"\"\n        A decorator factory for registering message handlers.\n\n        The decorated function will be called to process messages that match\n        the criteria specified in the config.\n\n        Args:\n            config: A dictionary with configuration for the handler,\n                    e.g., {'topic': 'my-topic', 'codes': ['code1', 'code2']}.\n        Returns:\n            A decorator that registers the processing function.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def listen(self):\n        \"\"\"\n        Starts the consumer to listen for messages and dispatch them to the\n        registered handlers.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def start(self) -&gt; None:\n        \"\"\"Initializes and starts the underlying Kafka consumer.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def stop(self) -&gt; None:\n        \"\"\"Stops the underlying Kafka consumer.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"consumer/#src.consumers.domain.ports.base_consumer.BaseConsumer.__init__","title":"<code>__init__(config, key_deserializer=None, value_deserializer=None)</code>","text":"<p>Initializes the BaseConsumer with configuration and deserializers.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The Kafka consumer configuration.</p> required <code>key_deserializer</code> <code>Optional[Callable]</code> <p>Optional function to deserialize message keys.</p> <code>None</code> <code>value_deserializer</code> <code>Optional[Callable]</code> <p>Optional function to deserialize message values.</p> <code>None</code> Source code in <code>src/consumers/domain/ports/base_consumer.py</code> <pre><code>def __init__(\n    self,\n    config: Dict[str, Any],\n    key_deserializer: Optional[Callable] = None,\n    value_deserializer: Optional[Callable] = None,\n):\n    \"\"\"\n    Initializes the BaseConsumer with configuration and deserializers.\n\n    Args:\n        config: The Kafka consumer configuration.\n        key_deserializer: Optional function to deserialize message keys.\n        value_deserializer: Optional function to deserialize message values.\n    \"\"\"\n    self.config = config\n    self._key_deserializer = key_deserializer\n    self._value_deserializer = value_deserializer\n</code></pre>"},{"location":"consumer/#src.consumers.domain.ports.base_consumer.BaseConsumer.get","title":"<code>get(config)</code>  <code>abstractmethod</code>","text":"<p>A decorator factory for registering message handlers.</p> <p>The decorated function will be called to process messages that match the criteria specified in the config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>A dictionary with configuration for the handler,     e.g., {'topic': 'my-topic', 'codes': ['code1', 'code2']}.</p> required <p>Returns:     A decorator that registers the processing function.</p> Source code in <code>src/consumers/domain/ports/base_consumer.py</code> <pre><code>@abstractmethod\ndef get(\n    self,\n    config: Dict[str, Any],\n) -&gt; Callable[[Callable[[ConsumerRecord], Coroutine]], None]:\n    \"\"\"\n    A decorator factory for registering message handlers.\n\n    The decorated function will be called to process messages that match\n    the criteria specified in the config.\n\n    Args:\n        config: A dictionary with configuration for the handler,\n                e.g., {'topic': 'my-topic', 'codes': ['code1', 'code2']}.\n    Returns:\n        A decorator that registers the processing function.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"consumer/#src.consumers.domain.ports.base_consumer.BaseConsumer.listen","title":"<code>listen()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Starts the consumer to listen for messages and dispatch them to the registered handlers.</p> Source code in <code>src/consumers/domain/ports/base_consumer.py</code> <pre><code>@abstractmethod\nasync def listen(self):\n    \"\"\"\n    Starts the consumer to listen for messages and dispatch them to the\n    registered handlers.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"consumer/#src.consumers.domain.ports.base_consumer.BaseConsumer.start","title":"<code>start()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Initializes and starts the underlying Kafka consumer.</p> Source code in <code>src/consumers/domain/ports/base_consumer.py</code> <pre><code>@abstractmethod\nasync def start(self) -&gt; None:\n    \"\"\"Initializes and starts the underlying Kafka consumer.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"consumer/#src.consumers.domain.ports.base_consumer.BaseConsumer.stop","title":"<code>stop()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Stops the underlying Kafka consumer.</p> Source code in <code>src/consumers/domain/ports/base_consumer.py</code> <pre><code>@abstractmethod\nasync def stop(self) -&gt; None:\n    \"\"\"Stops the underlying Kafka consumer.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"consumer/#kafka-adapter","title":"Kafka Adapter","text":"<p>The <code>KafkaBaseConsumerAdapter</code> is the concrete implementation using <code>aiokafka</code>. It handles the low-level details of connecting to Kafka, polling for messages, committing offsets, and handling rebalances.</p> <p>               Bases: <code>BaseConsumer</code></p> <p>Manages Kafka consumption, handling multiple topics and dynamically dispatching messages to registered handlers.</p> Source code in <code>src/consumers/infrastructure/adapters/base_consumer_adapter.py</code> <pre><code>class KafkaBaseConsumerAdapter(BaseConsumer):\n    \"\"\"\n    Manages Kafka consumption, handling multiple topics and dynamically\n    dispatching messages to registered handlers.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Dict[str, Any],\n        key_deserializer: Optional[Callable] = None,\n        value_deserializer: Optional[Callable] = None,\n    ):\n        super().__init__(config, key_deserializer, value_deserializer)\n\n        self._consumer: Optional[AIOKafkaConsumer] = None\n        self._running = False\n        self._rebalance_in_progress = False\n        self._handlers: Dict[str, Dict[str, Handler]] = defaultdict(dict)\n        self._topics: Set[str] = set()\n\n    def get(\n        self, config: Dict[str, str | list[str]]\n    ) -&gt; Callable[[Callable[..., Coroutine]], Callable[..., Coroutine]]:\n        \"\"\"\n        Decorator factory to register a function as a message handler.\n\n        Args:\n            config: A dictionary containing 'topic' and optional 'codes' to filter messages.\n\n        Returns:\n            A decorator that registers the function.\n        \"\"\"\n        topic = config.get(\"topic\")\n        codes = config.get(\"codes\", [])\n\n        if topic:\n            self._topics.add(topic)\n\n        def decorator(func: Callable[..., Coroutine]):\n\n            if isinstance(codes, str):\n                codes_list = [codes]\n            elif isinstance(codes, list):\n                codes_list = codes\n            else:\n                codes_list = []\n\n            handler = Handler(func=func, topic=topic, codes=codes_list)\n\n            if not codes:\n                \"\"\" Default case: No specific codes, register for all messages on the topic \"\"\"\n                self._handlers[topic][\"__default__\"] = handler\n            else:\n                \"\"\" Case: Index the same handler for each code it handles \"\"\"\n                for code in codes:\n                    self._handlers[topic][code] = handler\n\n            log.debug(\n                f\"Handler registered for topic '{topic}' with codes {codes or 'ALL'}\"\n            )\n\n            return func\n\n        return decorator\n\n    async def start(self) -&gt; None:\n        \"\"\"Initializes the Kafka consumer and subscribes to registered topics.\"\"\"\n        if self._consumer:\n            return\n\n        if not self._topics:\n            log.warning(\"No topics registered. The consumer will not start.\")\n            return\n\n        log.debug(\"Starting Kafka Consumer Manager...\")\n        self._consumer = AIOKafkaConsumer(\n            **self.config,\n            key_deserializer=self._key_deserializer,\n            value_deserializer=self._value_deserializer,\n        )\n        await self._consumer.start()\n\n        listener = RebalanceHandler(self)\n        self._consumer.subscribe(topics=list(self._topics), listener=listener)\n\n        self._running = True\n        log.info(\"Consumer Manager started successfully for topics: %s\", self._topics)\n\n    async def _commit_offset(self, msg):\n        try:\n            tp = TopicPartition(msg.topic, msg.partition)\n            await self._consumer.commit({tp: msg.offset + 1})\n        except (CommitFailedError, UnknownMemberIdError, IllegalGenerationError):\n            log.error(\n                \"Commit failed, rebalance likely in progress.\",\n                exc_info=True,\n                extra={\"partition\": msg.partition, \"offset\": msg.offset},\n            )\n            self._rebalance_in_progress = True\n\n    async def _process_message(self, msg: ConsumerRecord, **kwargs):\n        \"\"\"Processes a single message by dispatching it to relevant handlers.\"\"\"\n\n        if self._rebalance_in_progress:\n            log.warning(\"Rebalance in progress, skipping message processing.\")\n            return\n\n        msg_code = \"__default__\"\n        if isinstance(msg.value, dict) and \"code\" in msg.value:\n            msg_code = msg.value.get(\"code\")\n\n        topic_handlers = self._handlers.get(msg.topic, {})\n        handler = topic_handlers.get(msg_code)\n        if not handler:\n            log.warning(\n                f\"No handler found for topic {msg.topic} code {msg_code}. Skipping.\"\n            )\n            await self._commit_offset(msg)\n            return\n\n        try:\n            await handler.func(msg, **kwargs)\n            await self._commit_offset(msg)\n        except (\n            CommitFailedError,\n            UnknownMemberIdError,\n            IllegalGenerationError,\n        ) as e:\n            log.error(\n                \"Commit failed, rebalance might be in progress: %s\",\n                e,\n                extra={\"partition\": msg.partition, \"offset\": msg.offset},\n            )\n            self._rebalance_in_progress = True  # Signal rebalance\n            return\n        except Exception as e:\n            log.error(\n                \"Unexpected error during message processing by '%s': %s\",\n                handler.func.__name__,\n                e,\n                exc_info=True,\n                extra={\"partition\": msg.partition, \"offset\": msg.offset},\n            )\n            return\n\n    async def listen(self, **kwargs):\n        \"\"\"The main consumer loop that fetches and processes messages.\"\"\"\n        if not self._running or not self._consumer:\n            log.error(\"Consumer is not running. Call start() before listening.\")\n            return\n\n        log.info(\"Consumer is now listening for messages...\")\n        try:\n            while self._running:\n                if self._rebalance_in_progress:\n                    log.warning(\"Rebalance in progress, pausing message fetching.\")\n                    await asyncio.sleep(1)\n                    continue\n\n                try:\n                    result = await self._consumer.getmany(timeout_ms=1000)\n                    for tp, messages in result.items():\n                        if tp not in self._consumer.assignment():\n                            continue\n                        for msg in messages:\n                            await self._process_message(msg, **kwargs)\n                            if self._rebalance_in_progress:\n                                break\n                        if self._rebalance_in_progress:\n                            break\n\n                except Exception:\n                    log.error(\"Critical error in consumer loop.\", exc_info=True)\n                    # Simple backoff\n                    await asyncio.sleep(2)\n\n        finally:\n            await self.stop()\n\n    async def stop(self):\n        \"\"\"Stops the consumer and cleans up resources.\"\"\"\n        self._running = False\n        if self._consumer:\n            log.info(\"Stopping consumer manager...\")\n            await self._consumer.stop()\n            self._consumer = None\n            log.info(\"Consumer manager stopped.\")\n\n    async def __aenter__(self):\n        \"\"\"Enables usage with 'async with' statement.\"\"\"\n        await self.start()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Cleans up resources upon exiting 'async with' block.\"\"\"\n        await self.stop()\n</code></pre>"},{"location":"consumer/#src.consumers.infrastructure.adapters.base_consumer_adapter.KafkaBaseConsumerAdapter.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Enables usage with 'async with' statement.</p> Source code in <code>src/consumers/infrastructure/adapters/base_consumer_adapter.py</code> <pre><code>async def __aenter__(self):\n    \"\"\"Enables usage with 'async with' statement.\"\"\"\n    await self.start()\n    return self\n</code></pre>"},{"location":"consumer/#src.consumers.infrastructure.adapters.base_consumer_adapter.KafkaBaseConsumerAdapter.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Cleans up resources upon exiting 'async with' block.</p> Source code in <code>src/consumers/infrastructure/adapters/base_consumer_adapter.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Cleans up resources upon exiting 'async with' block.\"\"\"\n    await self.stop()\n</code></pre>"},{"location":"consumer/#src.consumers.infrastructure.adapters.base_consumer_adapter.KafkaBaseConsumerAdapter.get","title":"<code>get(config)</code>","text":"<p>Decorator factory to register a function as a message handler.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, str | list[str]]</code> <p>A dictionary containing 'topic' and optional 'codes' to filter messages.</p> required <p>Returns:</p> Type Description <code>Callable[[Callable[..., Coroutine]], Callable[..., Coroutine]]</code> <p>A decorator that registers the function.</p> Source code in <code>src/consumers/infrastructure/adapters/base_consumer_adapter.py</code> <pre><code>def get(\n    self, config: Dict[str, str | list[str]]\n) -&gt; Callable[[Callable[..., Coroutine]], Callable[..., Coroutine]]:\n    \"\"\"\n    Decorator factory to register a function as a message handler.\n\n    Args:\n        config: A dictionary containing 'topic' and optional 'codes' to filter messages.\n\n    Returns:\n        A decorator that registers the function.\n    \"\"\"\n    topic = config.get(\"topic\")\n    codes = config.get(\"codes\", [])\n\n    if topic:\n        self._topics.add(topic)\n\n    def decorator(func: Callable[..., Coroutine]):\n\n        if isinstance(codes, str):\n            codes_list = [codes]\n        elif isinstance(codes, list):\n            codes_list = codes\n        else:\n            codes_list = []\n\n        handler = Handler(func=func, topic=topic, codes=codes_list)\n\n        if not codes:\n            \"\"\" Default case: No specific codes, register for all messages on the topic \"\"\"\n            self._handlers[topic][\"__default__\"] = handler\n        else:\n            \"\"\" Case: Index the same handler for each code it handles \"\"\"\n            for code in codes:\n                self._handlers[topic][code] = handler\n\n        log.debug(\n            f\"Handler registered for topic '{topic}' with codes {codes or 'ALL'}\"\n        )\n\n        return func\n\n    return decorator\n</code></pre>"},{"location":"consumer/#src.consumers.infrastructure.adapters.base_consumer_adapter.KafkaBaseConsumerAdapter.listen","title":"<code>listen(**kwargs)</code>  <code>async</code>","text":"<p>The main consumer loop that fetches and processes messages.</p> Source code in <code>src/consumers/infrastructure/adapters/base_consumer_adapter.py</code> <pre><code>async def listen(self, **kwargs):\n    \"\"\"The main consumer loop that fetches and processes messages.\"\"\"\n    if not self._running or not self._consumer:\n        log.error(\"Consumer is not running. Call start() before listening.\")\n        return\n\n    log.info(\"Consumer is now listening for messages...\")\n    try:\n        while self._running:\n            if self._rebalance_in_progress:\n                log.warning(\"Rebalance in progress, pausing message fetching.\")\n                await asyncio.sleep(1)\n                continue\n\n            try:\n                result = await self._consumer.getmany(timeout_ms=1000)\n                for tp, messages in result.items():\n                    if tp not in self._consumer.assignment():\n                        continue\n                    for msg in messages:\n                        await self._process_message(msg, **kwargs)\n                        if self._rebalance_in_progress:\n                            break\n                    if self._rebalance_in_progress:\n                        break\n\n            except Exception:\n                log.error(\"Critical error in consumer loop.\", exc_info=True)\n                # Simple backoff\n                await asyncio.sleep(2)\n\n    finally:\n        await self.stop()\n</code></pre>"},{"location":"consumer/#src.consumers.infrastructure.adapters.base_consumer_adapter.KafkaBaseConsumerAdapter.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Initializes the Kafka consumer and subscribes to registered topics.</p> Source code in <code>src/consumers/infrastructure/adapters/base_consumer_adapter.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Initializes the Kafka consumer and subscribes to registered topics.\"\"\"\n    if self._consumer:\n        return\n\n    if not self._topics:\n        log.warning(\"No topics registered. The consumer will not start.\")\n        return\n\n    log.debug(\"Starting Kafka Consumer Manager...\")\n    self._consumer = AIOKafkaConsumer(\n        **self.config,\n        key_deserializer=self._key_deserializer,\n        value_deserializer=self._value_deserializer,\n    )\n    await self._consumer.start()\n\n    listener = RebalanceHandler(self)\n    self._consumer.subscribe(topics=list(self._topics), listener=listener)\n\n    self._running = True\n    log.info(\"Consumer Manager started successfully for topics: %s\", self._topics)\n</code></pre>"},{"location":"consumer/#src.consumers.infrastructure.adapters.base_consumer_adapter.KafkaBaseConsumerAdapter.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stops the consumer and cleans up resources.</p> Source code in <code>src/consumers/infrastructure/adapters/base_consumer_adapter.py</code> <pre><code>async def stop(self):\n    \"\"\"Stops the consumer and cleans up resources.\"\"\"\n    self._running = False\n    if self._consumer:\n        log.info(\"Stopping consumer manager...\")\n        await self._consumer.stop()\n        self._consumer = None\n        log.info(\"Consumer manager stopped.\")\n</code></pre>"},{"location":"consumer/#router","title":"Router","text":"<p>The <code>ConsumerRouter</code> allows you to organize your message handlers in a modular way, similar to <code>APIRouter</code> in FastAPI.</p> Source code in <code>src/consumers/router.py</code> <pre><code>class ConsumerRouter:\n    def __init__(self):\n        self._handlers: List[Tuple[Dict, Callable[..., Coroutine]]] = []\n\n    def get(self, config: Dict):\n        def decorator(func: Callable[..., Coroutine]):\n            self._handlers.append((config, func))\n            return func\n\n        return decorator\n\n    def register_handlers(self, consumer):\n        for config, func in self._handlers:\n            consumer.get(config)(func)\n\n    def include_router(self, router: \"ConsumerRouter\"):\n        self._handlers.extend(router._handlers)\n</code></pre>"},{"location":"consumer/#rebalance-handler","title":"Rebalance Handler","text":"<p>The <code>RebalanceHandler</code> listens for partition assignment changes during a consumer group rebalance. It ensures that message processing is paused/resumed correctly during these events to avoid inconsistencies.</p> <p>               Bases: <code>ConsumerRebalanceListener</code></p> <p>A listener that handles partition assignment changes during a consumer group rebalance. It signals the consumer manager when a rebalance is in progress to allow the manager to pause processing.</p> Source code in <code>src/consumers/infrastructure/adapters/base_rebalance_listener.py</code> <pre><code>class RebalanceHandler(ConsumerRebalanceListener):\n    \"\"\"\n    A listener that handles partition assignment changes during a consumer group\n    rebalance. It signals the consumer manager when a rebalance is in progress\n    to allow the manager to pause processing.\n    \"\"\"\n\n    def __init__(self, consumer_manager):\n        \"\"\"\n        Args:\n            consumer_manager: An instance of the ConsumerManager.\n        \"\"\"\n        self.consumer_manager = consumer_manager\n\n    async def on_partitions_revoked(self, revoked):\n        \"\"\"\n        Called when partitions are revoked from the consumer.\n        Sets the 'rebalance_in_progress' flag to True.\n        \"\"\"\n        log.warning(f\"Rebalance detected: Revoking partitions {revoked}\")\n        self.consumer_manager._rebalance_in_progress = True\n\n    async def on_partitions_assigned(self, assigned):\n        \"\"\"\n        Called when new partitions are assigned to the consumer.\n        Sets the 'rebalance_in_progress' flag to False.\n        \"\"\"\n        log.info(f\"Rebalance detected: Assigned partitions {assigned}\")\n        self.consumer_manager._rebalance_in_progress = False\n</code></pre>"},{"location":"consumer/#src.consumers.infrastructure.adapters.base_rebalance_listener.RebalanceHandler.__init__","title":"<code>__init__(consumer_manager)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>consumer_manager</code> <p>An instance of the ConsumerManager.</p> required Source code in <code>src/consumers/infrastructure/adapters/base_rebalance_listener.py</code> <pre><code>def __init__(self, consumer_manager):\n    \"\"\"\n    Args:\n        consumer_manager: An instance of the ConsumerManager.\n    \"\"\"\n    self.consumer_manager = consumer_manager\n</code></pre>"},{"location":"consumer/#src.consumers.infrastructure.adapters.base_rebalance_listener.RebalanceHandler.on_partitions_assigned","title":"<code>on_partitions_assigned(assigned)</code>  <code>async</code>","text":"<p>Called when new partitions are assigned to the consumer. Sets the 'rebalance_in_progress' flag to False.</p> Source code in <code>src/consumers/infrastructure/adapters/base_rebalance_listener.py</code> <pre><code>async def on_partitions_assigned(self, assigned):\n    \"\"\"\n    Called when new partitions are assigned to the consumer.\n    Sets the 'rebalance_in_progress' flag to False.\n    \"\"\"\n    log.info(f\"Rebalance detected: Assigned partitions {assigned}\")\n    self.consumer_manager._rebalance_in_progress = False\n</code></pre>"},{"location":"consumer/#src.consumers.infrastructure.adapters.base_rebalance_listener.RebalanceHandler.on_partitions_revoked","title":"<code>on_partitions_revoked(revoked)</code>  <code>async</code>","text":"<p>Called when partitions are revoked from the consumer. Sets the 'rebalance_in_progress' flag to True.</p> Source code in <code>src/consumers/infrastructure/adapters/base_rebalance_listener.py</code> <pre><code>async def on_partitions_revoked(self, revoked):\n    \"\"\"\n    Called when partitions are revoked from the consumer.\n    Sets the 'rebalance_in_progress' flag to True.\n    \"\"\"\n    log.warning(f\"Rebalance detected: Revoking partitions {revoked}\")\n    self.consumer_manager._rebalance_in_progress = True\n</code></pre>"},{"location":"producer/","title":"Producer","text":"<p>The Producer component in Kafkify allows you to send messages to Kafka topics.</p>"},{"location":"producer/#workflow","title":"Workflow","text":"<p>The following sequence diagram illustrates how the Producer is initialized and how messages are sent.</p> <pre><code>sequenceDiagram\n    participant Client as Client/Service\n    participant App as FastAPI App\n    participant Adapter as Kafka Producer Adapter\n    participant Kafka as Kafka Broker\n\n    Note over App, Adapter: Startup Phase\n    App-&gt;&gt;Adapter: Init Config\n    App-&gt;&gt;Adapter: Start()\n    Adapter-&gt;&gt;Kafka: Connect\n\n    Note over Client, Kafka: Sending Phase\n    Client-&gt;&gt;App: Trigger Event (e.g., API Call)\n    App-&gt;&gt;Adapter: send(topic, value, key)\n    Adapter-&gt;&gt;Kafka: Publish Message\n    Kafka--&gt;&gt;Adapter: Acknowledgment (Ack)\n    Adapter--&gt;&gt;App: Success\n    App--&gt;&gt;Client: Response\n\n    Note over App, Adapter: Shutdown Phase\n    App-&gt;&gt;Adapter: Stop()\n    Adapter-&gt;&gt;Kafka: Close Connection</code></pre>"},{"location":"producer/#usage","title":"Usage","text":"<p>To use the producer:</p> <ol> <li>Configure: Create a configuration dictionary with attributes supported by aiokafka.</li> <li>Instantiate: Create an instance of <code>KafkaBaseProducerAdapter</code> with the config.</li> <li>Start: Call <code>await producer.start()</code> to connect to Kafka.</li> <li>Send: Call <code>await producer.send(topic, value, key)</code> to publish messages.</li> <li>Stop: Call <code>await producer.stop()</code> to close the connection.</li> </ol> <p>See the Producer Example for a complete walkthrough.</p>"},{"location":"producer/#components","title":"Components","text":""},{"location":"producer/#configuration","title":"Configuration","text":"<p>The configuration is passed as a dictionary. It supports all arguments accepted by <code>aiokafka.AIOKafkaProducer</code>.</p> <p>Common options include: - <code>bootstrap_servers</code>: list of 'host:port' strings or a single comma-separated string. - <code>client_id</code>: client identifier for traceability. - <code>acks</code>: acknowledgment level ('all', 0, 1). - <code>request_timeout_ms</code>: timeout for requests in milliseconds.</p> <p>Refer to the aiokafka documentation for a full list of parameters.</p>"},{"location":"producer/#base-producer","title":"Base Producer","text":"<p>The <code>BaseProducer</code> defines the interface for all producer implementations.</p> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for a message bus producer.</p> Source code in <code>src/producers/domain/ports/base_producer.py</code> <pre><code>class BaseProducer(ABC):\n    \"\"\"\n    Abstract base class for a message bus producer.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Dict[str, Any],\n        key_deserializer: Optional[Callable] = None,\n        value_deserializer: Optional[Callable] = None,\n    ):\n        \"\"\"\n        Initializes the BaseProducer with configuration and deserializers.\n\n        Args:\n            config: The Kafka producer configuration.\n            key_deserializer: Optional function to deserialize message keys.\n            value_deserializer: Optional function to deserialize message values.\n        \"\"\"\n        self.config = config\n        self._key_deserializer = key_deserializer\n        self._value_deserializer = value_deserializer\n\n    @abstractmethod\n    async def start(self) -&gt; None:\n        \"\"\"Initializes and starts the underlying Kafka producer.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def stop(self) -&gt; None:\n        \"\"\"Stops the underlying Kafka producer.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def send(\n        self,\n        topic: str,\n        key: SerializableKey,\n        value: SerializableValue,\n    ) -&gt; None:\n        \"\"\"Sends a message to the specified topic.\n        Args:\n            topic: The Kafka topic to send the message to.\n            key: The key of the message, either as a string, bytes, or a JSON-serializable object.\n            value: The value of the message, either as a string, bytes, or a JSON-serializable object.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"producer/#src.producers.domain.ports.base_producer.BaseProducer.__init__","title":"<code>__init__(config, key_deserializer=None, value_deserializer=None)</code>","text":"<p>Initializes the BaseProducer with configuration and deserializers.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>The Kafka producer configuration.</p> required <code>key_deserializer</code> <code>Optional[Callable]</code> <p>Optional function to deserialize message keys.</p> <code>None</code> <code>value_deserializer</code> <code>Optional[Callable]</code> <p>Optional function to deserialize message values.</p> <code>None</code> Source code in <code>src/producers/domain/ports/base_producer.py</code> <pre><code>def __init__(\n    self,\n    config: Dict[str, Any],\n    key_deserializer: Optional[Callable] = None,\n    value_deserializer: Optional[Callable] = None,\n):\n    \"\"\"\n    Initializes the BaseProducer with configuration and deserializers.\n\n    Args:\n        config: The Kafka producer configuration.\n        key_deserializer: Optional function to deserialize message keys.\n        value_deserializer: Optional function to deserialize message values.\n    \"\"\"\n    self.config = config\n    self._key_deserializer = key_deserializer\n    self._value_deserializer = value_deserializer\n</code></pre>"},{"location":"producer/#src.producers.domain.ports.base_producer.BaseProducer.send","title":"<code>send(topic, key, value)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Sends a message to the specified topic. Args:     topic: The Kafka topic to send the message to.     key: The key of the message, either as a string, bytes, or a JSON-serializable object.     value: The value of the message, either as a string, bytes, or a JSON-serializable object.</p> Source code in <code>src/producers/domain/ports/base_producer.py</code> <pre><code>@abstractmethod\nasync def send(\n    self,\n    topic: str,\n    key: SerializableKey,\n    value: SerializableValue,\n) -&gt; None:\n    \"\"\"Sends a message to the specified topic.\n    Args:\n        topic: The Kafka topic to send the message to.\n        key: The key of the message, either as a string, bytes, or a JSON-serializable object.\n        value: The value of the message, either as a string, bytes, or a JSON-serializable object.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"producer/#src.producers.domain.ports.base_producer.BaseProducer.start","title":"<code>start()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Initializes and starts the underlying Kafka producer.</p> Source code in <code>src/producers/domain/ports/base_producer.py</code> <pre><code>@abstractmethod\nasync def start(self) -&gt; None:\n    \"\"\"Initializes and starts the underlying Kafka producer.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"producer/#src.producers.domain.ports.base_producer.BaseProducer.stop","title":"<code>stop()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Stops the underlying Kafka producer.</p> Source code in <code>src/producers/domain/ports/base_producer.py</code> <pre><code>@abstractmethod\nasync def stop(self) -&gt; None:\n    \"\"\"Stops the underlying Kafka producer.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"producer/#kafka-adapter","title":"Kafka Adapter","text":"<p>The <code>KafkaBaseProducerAdapter</code> is the concrete implementation using <code>aiokafka</code>.</p> <p>               Bases: <code>BaseProducer</code></p> Source code in <code>src/producers/infrastructure/adapters/base_producer_adapter.py</code> <pre><code>class KafkaBaseProducerAdapter(BaseProducer):\n    def __init__(\n        self,\n        config: Dict[str, Any],\n        key_serializer: Optional[Callable] = None,\n        value_serializer: Optional[Callable] = None,\n    ):\n        super().__init__(config, key_serializer, value_serializer)\n\n        self._producer: Optional[AIOKafkaProducer] = None\n\n    async def start(self) -&gt; None:\n        if self._producer:\n            return\n\n        log.debug(\"Starting Kafka Producer...\")\n        self._producer = AIOKafkaProducer(\n            **self.config,\n            value_serializer=self._value_deserializer,\n            key_serializer=self._key_deserializer,\n        )\n        await self._producer.start()\n        log.debug(\"Kafka Producer started successfully.\")\n\n    async def stop(self) -&gt; None:\n        if self._producer:\n            log.debug(\"Stopping Kafka Producer...\")\n            await self._producer.stop()\n            self._producer = None\n            log.debug(\"Kafka Producer stopped.\")\n\n    async def send(\n        self,\n        topic: str,\n        value: SerializableValue,\n        key: SerializableKey = None,\n    ) -&gt; None:\n        if not self._producer:\n            raise RuntimeError(\n                \"The producer is not started. Call 'start()' before sending messages.\"\n            )\n\n        try:\n            await self._producer.send_and_wait(\n                topic=topic,\n                value=value,\n                key=key,\n            )\n\n            log.debug(\n                \"Message sent successfully.\",\n                extra={\n                    \"topic\": topic,\n                },\n            )\n            return True\n\n        except KafkaError as e:\n            log.error(\n                f\"Kafka error fatal: {e._get_error_name()}\",\n                exc_info=True,\n                extra={\"topic\": topic, \"key\": key},\n            )\n            raise Exception(\"Failed to send message to Kafka\")\n        except Exception:\n            log.error(\n                \"Unexpected error when sending message to Kafka\",\n                exc_info=True,\n                extra={\"topic\": topic, \"key\": key},\n            )\n            raise Exception(\"Unexpected error when sending message to Kafka\")\n\n    async def __aenter__(self):\n        await self.start()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.stop()\n</code></pre>"},{"location":"examples/consumer/","title":"Consumer Example","text":"<p>This example demonstrates how to set up a Kafka Consumer within a FastAPI application using Kafkify.</p>"},{"location":"examples/consumer/#structure","title":"Structure","text":"<p>The example consists of: -   Main Application (<code>main.py</code>): Sets up the FastAPI app, configures the consumer, and manages the consumer lifecycle. -   Router (<code>router.py</code>): Defines the message handlers using <code>ConsumerRouter</code>.</p>"},{"location":"examples/consumer/#defining-handlers","title":"Defining Handlers","text":"<p>You can define handlers using the <code>ConsumerRouter</code>. This allows you to group related event handlers.</p> src/examples/consumer_example/entrypoints/api/routers/events/router.py<pre><code># ... imports ...\nfrom src.consumers.router import ConsumerRouter\n\nmain_event_router = ConsumerRouter()\n\n@main_event_router.get({\"topic\": \"people-events\", \"codes\": [\"PERSON_CREATED\"]})\nasync def handle_person_created(msg: ConsumerRecord):\n    data = msg.value\n    # Process the message\n    print(f\"Person created: {data}\")\n\n@main_event_router.get({\"topic\": \"purchase-events\"})\nasync def handle_purchase(msg: ConsumerRecord):\n    # Handles all messages on 'purchase-events' regardless of code\n    print(f\"Purchase received: {msg.value}\")\n</code></pre>"},{"location":"examples/consumer/#organizing-handlers","title":"Organizing Handlers","text":"<p>For larger applications, it is recommended to split your handlers into multiple files (e.g., by domain or topic) and use a main router to aggregate them. This main router acts as a central registry or factory.</p> <p>Example of an aggregating router:</p> src/examples/consumer_example/entrypoints/api/routers/events/router.py<pre><code>from src.consumers.router import ConsumerRouter\nfrom src.examples.consumer_example.entrypoints.api.routers.events import (\n    people_event,\n    purchase_event,\n)\n\nmain_event_router = ConsumerRouter()\n\n# Register all domain event routers here\nmain_event_router.include_router(people_event.router)\nmain_event_router.include_router(purchase_event.router)\n</code></pre>"},{"location":"examples/consumer/#application-setup","title":"Application Setup","text":"<p>In your <code>main.py</code>, you initialize the consumer and register the router.</p> src/examples/consumer_example/main.py<pre><code># ... imports ...\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # 1. Load Configuration\n    kafka_config = config_manager.get_property(\"kafka\")\n\n    # 2. Define Consumer Config (Dictionary)\n    consumer_config = {\n        \"bootstrap_servers\": kafka_config.get(\"bootstrap_servers\", \"localhost:9092\"),\n        \"group_id\": \"example-consumer-group\",\n        \"auto_offset_reset\": \"earliest\",\n        \"enable_auto_commit\": False,\n        # Add other aiokafka supported options here\n    }\n\n    # 3. Create Consumer Adapter\n    consumer = KafkaBaseConsumerAdapter(\n        config=consumer_config,\n        key_deserializer=default_deserializer,\n        value_deserializer=default_deserializer,\n    )\n\n    # 4. Register Handlers\n    main_event_router.register_handlers(consumer)\n\n    # 5. Start Consumer\n    await consumer.start()\n\n    # 6. Start Listening Loop (in background)\n    consumer_task = asyncio.create_task(consumer.listen())\n\n    yield\n\n    # 7. Stop Consumer (graceful shutdown)\n    await consumer.stop()\n    try:\n        await consumer_task\n    except asyncio.CancelledError:\n        pass\n\napp = FastAPI(lifespan=lifespan)\n</code></pre>"},{"location":"examples/consumer/#running-the-example","title":"Running the Example","text":"<p>Make sure you have a Kafka broker running (e.g., via <code>docker-compose</code>).</p> <pre><code>uvicorn src.examples.consumer_example.main:app --reload --port 8001\n</code></pre>"},{"location":"examples/producer/","title":"Producer Example","text":"<p>This example demonstrates how to set up a Kafka Producer within a FastAPI application using Kafkify.</p>"},{"location":"examples/producer/#structure","title":"Structure","text":"<p>The example consists of: -   Main Application (<code>main.py</code>): Sets up the FastAPI app, configures the producer, and manages the producer lifecycle. -   Service (<code>person_notifier_service.py</code>): Uses the producer to send messages via an API endpoint.</p>"},{"location":"examples/producer/#application-setup","title":"Application Setup","text":"<p>In your <code>main.py</code>, you initialize the producer and store it in the app state so it can be accessed by dependencies.</p> src/examples/producer_example/main.py<pre><code># ... imports ...\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # 1. Load Configuration\n    kafka_config = config_manager.get_property(\"kafka\")\n\n    # 2. Define Producer Config (Dictionary)\n    producer_config = {\n        \"bootstrap_servers\": kafka_config.get(\"bootstrap_servers\", \"localhost:9092\"),\n        \"client_id\": \"example-producer\",\n        \"acks\": \"all\",\n        # Add other aiokafka supported options here\n    }\n\n    # 3. Create Producer Adapter\n    producer = KafkaBaseProducerAdapter(\n        config=producer_config,\n        key_serializer=default_serializer,\n        value_serializer=default_serializer,\n    )\n\n    # 4. Start Producer\n    await producer.start()\n\n    # 5. Store in App State\n    app.state.producer = producer\n\n    yield\n\n    # 6. Stop Producer\n    await producer.stop()\n\napp = FastAPI(lifespan=lifespan)\n</code></pre>"},{"location":"examples/producer/#sending-messages","title":"Sending Messages","text":"<p>You can inject the producer into your API routes (e.g., using <code>Request.app.state.producer</code> or a dependency) and use the <code>send</code> method.</p> src/examples/producer_example/entrypoints/api/routers/person_notifier_service.py<pre><code># ... imports ...\nfrom fastapi import APIRouter, Depends\nfrom starlette.requests import Request\n\nrouter = APIRouter()\n\n@router.post(\"/person/notify\")\nasync def notify_person(\n    request: Request,\n    person_data: dict\n):\n    producer = request.app.state.producer\n\n    # Send message\n    await producer.send(\n        topic=\"people-events\",\n        value={\n            \"code\": \"PERSON_CREATED\",\n            \"data\": person_data\n        },\n        key=person_data.get(\"id\")\n    )\n\n    return {\"status\": \"Message sent\"}\n</code></pre>"},{"location":"examples/producer/#running-the-example","title":"Running the Example","text":"<p>Make sure you have a Kafka broker running.</p> <pre><code>uvicorn src.examples.producer_example.main:app --reload --port 8000\n</code></pre>"}]}